{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Кодирование производится с помощью BiLSTM (не принципиально)\n",
    "* Веса $a_i$ обычно в сумме равны $1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/attention.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируя веса, можно понимать стратегию получения результата (от каких частей контекста зависела каждая из частей выхода).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример визуализации весов $attention$ в задаче машинного перевода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/attention_matrix.png\" alt=\"Drawing\" style=\"width: 480px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовании BiLSTM каждый вектор $h_j$ хранит информацию о всей последовательности, но в наибольшей степени о $j$-м слове и его соседях.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее при текущем выходе $y_{t−1}$ (с вектором $s_{t−1}$) для вектора каждого входного слова $h_j$ считается $a_{tj}$ – вклад в генерацию следующего выходного вектора ($attention$):\n",
    "\n",
    "$$a_{tj} = \\frac {e^{e_{tj}}} {\\sum_k e^{e_{tk}}}$$\n",
    "\n",
    "где $e_{tj} = f (s_{t−1}, h_j )$ – модель выравнивания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "􏰀Модель выравнивания предсказывает то, насколько хорошо соотносятся входное слово в позиции j и выходное в позиции t (обычно это простая модель, например, однослойная сеть)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вход: вектор запроса и несколько пар векторов ключей и значений (ключ и значение обычно совпадают)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/multi-head-attention.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для запроса и каждого ключа считается вес (линейный слой)\n",
    "Значения суммируются с этими весами в итоговый вектор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея: обучать несколько $attention$, в надежде, что они станут отвечать за разные признаки слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты пропустим через однослойную сеть, получим на выходе один вектор той же размерности, что и входные вектора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Состоит из encoder и decoder, в каждом используются $multi$-$head\\ attention$ и полносвязные (свёрточные) слои (нет RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого слова в encoder формируется вектор на основе нескольких слоёв $multi$-$head\\ attention$, который передаётся в декодер."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "􏰀На основании векторов энкодера, а также выходных векторов для уже обработанных слов получается вектор для текущего слова. В обоих случаях используется $multi$-$head\\ attention$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/transformer.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Positional\\ encoding$ – дополнительный вектор признаков для каждого слова, представляющий собой набор значений синусов и косинусов с разными периодами от позиции слова в предложении"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В трансформере и в encoder, и в decoder можно использовать несколько последовательных слоёв $multi$-$head\\ attention$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для большей выразительности и качества добавляют полносвязные слои, а также дропаут, нормализацию по слою и residual connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важная особенность: обучение модели внутри предложения можно распараллелить, в отличие от RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$BERT\\ (Bidirectional\\ Encoder\\ Representations\\ from\\ Transformers)\\ -$ многослойный двунаправленный transformer-encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель BERT основана на архитектуре Transformer и является усовершенствованием модели GPT от OpenAI.\n",
    "Модель, в отличие от GPT является двунаправленной.\n",
    "\n",
    "Ссылка на оригинальную статью: https://arxiv.org/abs/1810.04805"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/bert_model.png\" alt=\"Drawing\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные параметры модели:\n",
    "* L – количество $transformer$-блоков \n",
    "* H – размер скрытого представления,\n",
    "* A – количество параллельных слоев в $multi$-$head$ $attention$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует две основные модели от Google:\n",
    "\n",
    "* $BERT_{BASE}$: \n",
    "    * L = 12\n",
    "    * H = 768\n",
    "    * A = 12, \n",
    "    * Total = 110M\n",
    "$$$$\n",
    "* $BERT_{LARGE}$: \n",
    "    * L = 24\n",
    "    * H = 1024\n",
    "    * A = 16, \n",
    "    * Total = 340M\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель обучается на двух задачах: \n",
    "* Masked Language Modeling.\n",
    "* Next sentence prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от обычного Language Modeling, где предсказывается каждое следующее слово на каждом шаге, в задаче Masked Language Modeling случайные токены заменяются на специальный токен [MASK] и предсказываются только эти \"замаскированные\" токены."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 80% времени обучения: Заменим слово на [MASK].\n",
    "    * my dog is hairy → my dog is [MASK]\n",
    "* 10% времени обучения: Заменим слово на другое случайное слово \n",
    "    * my dog is hairy → my dog is apple\n",
    "* 10% времени обучения: оставим предложение без изменений. Цель этого - сделать модель смещенной в сторону слова, которое действительно встречалось в корпусе в данном предложении. \n",
    "    * my dog is hairy → my dog is hairy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next sentence prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next sentence prediction $-$ задача бинарной классификации. По паре предложений требуется определить, является ли второе предложение продолжением первого."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input =[CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]\n",
    "    * Label = IsNext\n",
    "$$$$\n",
    "* Input =[CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]\n",
    "    * Label = NotNext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на эмбеддинги, получаемые с помощью модели BERT. \n",
    "\n",
    "Будем использовать модель $BERT_{base}$ из реализации transformers от HuggingFace. \n",
    "Ссылка на их гитхаб: https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры текстов взяты из туториала (https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank. [SEP]\n"
     ]
    }
   ],
   "source": [
    "text = \"Here is the sentence I want embeddings for.\"\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "print (marked_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'after', 'stealing', 'money', 'from', 'the', 'bank', 'vault', ',', 'the', 'bank', 'robber', 'was', 'seen', 'fishing', 'on', 'the', 'mississippi', 'river', 'bank', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['knight', 'lap', 'survey', 'ma', '##ow', 'noise', 'billy', '##ium', 'shooting', 'guide', 'bedroom', 'priest', 'resistance', 'motor', 'homes', 'sounded', 'giant', '##mer', '150', 'scenes']\n"
     ]
    }
   ],
   "source": [
    "print(list(tokenizer.vocab.keys())[5000:5020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS]', 101)\n",
      "('after', 2044)\n",
      "('stealing', 11065)\n",
      "('money', 2769)\n",
      "('from', 2013)\n",
      "('the', 1996)\n",
      "('bank', 2924)\n",
      "('vault', 11632)\n",
      "(',', 1010)\n",
      "('the', 1996)\n",
      "('bank', 2924)\n",
      "('robber', 27307)\n",
      "('was', 2001)\n",
      "('seen', 2464)\n",
      "('fishing', 5645)\n",
      "('on', 2006)\n",
      "('the', 1996)\n",
      "('mississippi', 5900)\n",
      "('river', 2314)\n",
      "('bank', 2924)\n",
      "('.', 1012)\n",
      "('[SEP]', 102)\n"
     ]
    }
   ],
   "source": [
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print (tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "segments_ids = [1] * len(tokenized_text)\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    out = model(tokens_tensor, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_layers = out[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_layers[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13\n",
      "Number of batches: 1\n",
      "Number of tokens: 22\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(encoded_layers))\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the 5th token in our sentence, select its feature values from layer 5.\n",
    "token_i = 5\n",
    "layer_i = 5\n",
    "vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in sequence: 22\n",
      "Number of layers per token: 13\n"
     ]
    }
   ],
   "source": [
    "# Convert the hidden state embeddings into single token vectors\n",
    "\n",
    "# Holds the list of 12 layer embeddings for each token\n",
    "# Will have the shape: [# tokens, # layers, # features]\n",
    "token_embeddings = [] \n",
    "\n",
    "# For each token in the sentence...\n",
    "for token_i in range(len(tokenized_text)):\n",
    "  \n",
    "  # Holds 12 layers of hidden states for each token \n",
    "    hidden_layers = [] \n",
    "  \n",
    "  # For each of the 12 layers...\n",
    "    for layer_i in range(len(encoded_layers)):\n",
    "    \n",
    "    # Lookup the vector for `token_i` in `layer_i`\n",
    "        vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "\n",
    "        hidden_layers.append(vec)\n",
    "    \n",
    "    token_embeddings.append(hidden_layers)\n",
    "\n",
    "# Sanity check the dimensions:\n",
    "print (\"Number of tokens in sequence:\", len(token_embeddings))\n",
    "print (\"Number of layers per token:\", len(token_embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_last_4_layers = [torch.cat((layer[-1], layer[-2], layer[-3], layer[-4]), 0) for layer in token_embeddings] # [number_of_tokens, 3072]\n",
    "\n",
    "summed_last_4_layers = [torch.sum(torch.stack(layer)[-4:], 0) for layer in token_embeddings] # [number_of_tokens, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding = torch.mean(encoded_layers[11], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final sentence embedding vector of shape:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 768)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Our final sentence embedding vector of shape:\"), sentence_embedding[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 after\n",
      "2 stealing\n",
      "3 money\n",
      "4 from\n",
      "5 the\n",
      "6 bank\n",
      "7 vault\n",
      "8 ,\n",
      "9 the\n",
      "10 bank\n",
      "11 robber\n",
      "12 was\n",
      "13 seen\n",
      "14 fishing\n",
      "15 on\n",
      "16 the\n",
      "17 mississippi\n",
      "18 river\n",
      "19 bank\n",
      "20 .\n",
      "21 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i,x in enumerate(tokenized_text):\n",
    "    print(i,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First fifteen values of 'bank' as in 'bank robber':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 2.7359, -2.5577, -1.3094,  0.6797,  1.6633,  1.7501, -2.7480,  0.0250,\n",
       "         0.1940, -1.4371,  0.1224, -0.5929, -0.7653,  2.2977, -4.7485])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"First fifteen values of 'bank' as in 'bank robber':\")\n",
    "summed_last_4_layers[10][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First fifteen values of 'bank' as in 'bank vault':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 3.3596, -2.9805, -1.5421,  0.7065,  2.0031,  0.6318, -2.9078,  1.6307,\n",
       "        -1.0581, -2.4467,  0.1520, -1.8649, -0.8663,  1.5591, -4.4090])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"First fifteen values of 'bank' as in 'bank vault':\")\n",
    "summed_last_4_layers[6][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First fifteen values of 'bank' as in 'river bank':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5266, -0.8895, -0.5152, -0.9298,  2.8334, -0.0527,  1.2063,  4.3940,\n",
       "        -0.3398, -1.9989,  2.6127,  0.3061, -1.0399,  1.0284, -3.5758])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"First fifteen values of 'bank' as in 'river bank':\")\n",
    "summed_last_4_layers[19][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compare \"bank\" as in \"bank robber\" to \"bank\" as in \"river bank\"\n",
    "different_bank = cosine_similarity(summed_last_4_layers[10].reshape(1,-1), summed_last_4_layers[19].reshape(1,-1))[0][0]\n",
    "\n",
    "# Compare \"bank\" as in \"bank robber\" to \"bank\" as in \"bank vault\" \n",
    "same_bank = cosine_similarity(summed_last_4_layers[10].reshape(1,-1), summed_last_4_layers[6].reshape(1,-1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of 'bank' as in 'bank robber' to 'bank' as in 'bank vault': 0.9386392\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity of 'bank' as in 'bank robber' to 'bank' as in 'bank vault':\",  same_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of 'bank' as in 'bank robber' to 'bank' as in 'river bank': 0.6932363\n"
     ]
    }
   ],
   "source": [
    "print (\"Similarity of 'bank' as in 'bank robber' to 'bank' as in 'river bank':\",  different_bank)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
